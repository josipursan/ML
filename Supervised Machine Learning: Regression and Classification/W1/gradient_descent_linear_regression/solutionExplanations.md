# Gradient descent - line by line explanation
This .md is used to explain `sol.py` line by line.  
`sol.py` contains linear regression with gradient descent.  
`import` commands will be explained where necessary.  
All lines of code will be given in apporpriate code snippets like this :  
```python
# This is an example of a python code block
```  
<br></br>  

### Relevant equations
$$
\begin{align}
J(w,b) = \frac{1}{2m} \sum_{i=1}^{m}(f_{w,b}(x^{i}) - y^{i})^{2}
\end{align}
$$

$$
\begin{align}
w = w - \alpha\frac{\partial}{\partial{w}}J(w,b) = w - \alpha \frac{1}{m}\sum_{i=1}^{i}(f_{w,b}(x^{i}) - y^{i})x^{i}
\end{align}
$$

$$
\begin{align}
b = b - \alpha\frac{\partial}{\partial{b}}J(w,b) = w - \alpha \frac{1}{m}\sum_{i=1}^{i}(f_{w,b}(x^{i}) - y^{i})
\end{align}
$$  
  
`sol.py` represents the first solution - it has not been cleaned up, and is here simply for posterity and future reference.  
Cleaner, tidier and more succint solutions will be pushed in the future.

## Explanation
```python
def f(x):
    return ((3*x)+8)  # y = f(x) = wx + b
```  
This function `f(x)` represents our true model, the real data which for given range of x values (inputs) will yield equal number of y values.  
<br></br>
  
```python
x_vals = np.arange(1.0, 15.0, 0.95) # generate values in range 1-15, using 0.95 as step between values
y_vals = f(x_vals)
```  
`x_vals` generates input values in range 1-15, using 0.95 as step  
`y_vals` is then generated by passing `x_vals` to `f(x)`  
Now we have our *x* (feature) and *y* (target) values, ie. our training set.   
<br></br>

```python
w = random.uniform(-100.0, 100.0)
b = random.uniform(-100.0, 100.0)
```  
Here we generate random values for w and b. Random value can be in range -100,100.  
This isn't really necessary - usually you start with *w* and *b* being 0.  
<br></br>

```python
allCosts = []
all_w_b_hypothesis = []
all_y_vals = []
all_w_b_hypothesis.append([w,b])
```  
Lists used to store all values during all iterations.  
This is used for plotting and debugging stuff - not important to the overall algorithm  
<br></br>

```python
y_vals_prediction = w*x_vals+b
all_y_vals.append(y_vals_prediction)
```
`y_vals_prediction` is a list/array storing latest predicted y values (y^, y-hat values).  
`y_vals_prediction` contains y-hat values predicted by the latest value of *w* and *b* parameters (ie. values for *w* and *b* computed in the previous iteration)  
<br></br>

```python
1   sum_cost_func = 0
2   for el in range(0, len(y_vals_prediction)):
3       sum_cost_func += (y_vals_prediction[el] - y_vals[el])**2
4   cost_func = (1/(2*(len(y_vals_prediction)))) * sum_cost_func
5   allCosts.append(cost_func)
```  
This code snippet computes the cost function for the lastest y-hat values.  
Line 2 (`for(...)`) represents the summation operator $\sum$ in equation `(1)`.  
Line 3 (`sum_cost_func += ...`) is used to sum up all of the elements that need to be summed up under summation operator $\sum$.  
&nbsp;&nbsp;&nbsp;This is the $(f_{w,b}(x^{i}) - y^{i})^{2}$ part in equation `(1)`.  
Line 4 (`cost_func = ...`) is the $\frac{1}{2m}  \sum_{i=1}^{i}(f_{w,b}(x^{i}) - y^{i})^{2}$ in equation `(1)`.  
&nbsp;&nbsp;&nbsp; $\frac{1}{2m}$ is `(1/(2*len(y_vals_prediction)))` in line 4; *m* represents here the number of elements, ie. number of elements in `y_vals_prediction`  
Line 5 is irrelevant in terms of grad desc algorithm; it is just a field used for additional debugging  
<br></br>

```python
if(cost_func < 0.01):
    print("\n\n\nLast computed cost is below set threshold!\nCost : {}\nExiting!\n\n".format(cost_func))
    plt.figure(1)
    plt.plot(x_vals, y_vals, label="Original data", color="red")    # let's check out our real model
    plt.plot(x_vals, y_vals_prediction, linestyle='dotted', color='teal', linewidth=3, label="BEST FIT")
    ...
```
This if check is used to check whether cost of the last generated model is less than some arbitrarily chosen value.  
Why? What does it do?  
It is used to cleanly exit the infinite while loop used to run gradient descent, ie. the continuous process of improving model by better *w* and *b* parameter predictions.  
If cost_func given by the last generated model is less than `0.01`, gradient descent is ended, and the last used *w* and *b* parameters are considered the right ones.  
Everything else written in this `if()` clause is irrelevant - just some stuff for plotting and debugging.

```python
# Update the terms
1 w_summation_temp = 0
2 b_summation_temp = 0
3 for el in range(0, len(y_vals_prediction)):
4    w_summation_temp += (y_vals_prediction[el] - y_vals[el])*x_vals[el]
5    b_summation_temp += (y_vals_prediction[el] - y_vals[el])

6 w_summation_temp *= 1/(len(y_vals))
7 b_summation_temp *= 1/(len(y_vals))
8 w_summation_temp = alpha*w_summation_temp
9 b_summation_temp = alpha*b_summation_temp
10 w = w - w_summation_temp
11 b = b - b_summation_temp
```  
This block represents how equations `(2)` and `(3)` are implemented in code.  
Equations `(2)` and `(3)` are terms representing how *w* and *b* are updated using the newly created model.  
  
A few lines above this code block we've computed the cost function of our latest model, and we've then checked whether cost is below a certain arbitrarily define threshold - if it is, it means our model is performing as we want; it it isn't, it means we need to update our *w* and *b* parameters to get a new model for the next iteration.  
  
`for(..)` loop on line 3, as well as its contents, are representative of what is happening under $\sum$ operator in equations `(2)` and `(3)`.  
  
Result of summation (lines 3,4,5) needs to be multiplied by *m* - total number of training examples, ie. `len(x_vals)`, or `len(y_vals)`, or `len(y_hat)`. For this we have code in lines `6` and `7`.  
  
Then in lines `(7)` and `(8)` the end result of all previous operations in this code block is multiplied by $\alpha$ - the learning rate (represents size of each gradient descent step).  
  
Lastly, in lines `(10)` and `(11)` we simultaneously update *w* and *b* parameters by subtracting all computations described above from the current ("old") *w* and *b* values.  
  
*Note* - first version of *sol.py* had a mistake. You forgot to multiply result of lines `3,4,5` by $\frac{1}{m}$  